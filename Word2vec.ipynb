{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общий конвейер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df_train = pandas.read_csv(\"data/train.csv\")\n",
    "df_test  = pandas.read_csv(\"data/test.csv\")\n",
    "df_val   = pandas.read_csv(\"data/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>оооо, пошёл я:d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>остальные, к слову, из твоего - я тоже не ви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>все, что вы не знали о кофе, но понтовались и ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>в последнее время не могу побороть свою лень. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ахаххаха,дададад:* ибо нехуй тут ей:d:*</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0                                   оооо, пошёл я:d       1\n",
       "1   1    остальные, к слову, из твоего - я тоже не ви...      0\n",
       "2   2  все, что вы не знали о кофе, но понтовались и ...      1\n",
       "3   3  в последнее время не могу побороть свою лень. ...      0\n",
       "4   4            ахаххаха,дададад:* ибо нехуй тут ей:d:*      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Токенизация\n",
    "Кодируем слова индексами (вычисленными по частоте употребления).\n",
    "Using russian http://www.ruscorpora.ru/en/\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer Убираем, потому что он режет символы пунктуации\n",
    "#### Step 1. Clear dataset. \n",
    "Select words one by one. Symbols are meaningful because of smiles and emotions.\n",
    "\n",
    "We will create a HashMap by using a Python dictionary to store the word frequencies of a book.\n",
    "A dictionary is an associative array (also known as hashes).\n",
    "Any key of the dictionary is associated, or mapped, to a value.\n",
    "The values of a dictionary can be any Python data type, so dictionaries are unordered key-value-pairs.\n",
    "\n",
    "By creating the dictionary, we will store the words as the keys and the value will represent the count. By doing this, we can retrieve any word without having to recount every single word.\n",
    "\n",
    "#### Step 2. Select meaningful words.\n",
    "#### Step 3. Calculate frequency of each word\n",
    "#### Step 4. Replace words by indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis:\n",
    "    max_words = 2000\n",
    "    batch_size = 32\n",
    "    epochs = 30\n",
    "    max_len = 15\n",
    "    cnstr = 'Trusted_Connection=yes;DRIVER={SQL Server};SERVER=GORDAPC;DATABASE=positive;UID=sa;PWD=49649952'\n",
    "    #'Trusted_Connection=yes;DRIVER={SQL Server};SERVER=GORDAPC\\MSSQLSERVER2017;DATABASE=InnaDB;UID=sa;PWD=49649952'\n",
    "    map = {}\n",
    "    x_train, y_train, x_test, y_test = ([] for i in range(4))\n",
    "\n",
    "    def data_generator(self, constring, query):\n",
    "        cnxn = pyodbc.connect(constring)\n",
    "        cursor = cnxn.cursor()\n",
    "        cursor.execute(query)\n",
    "\n",
    "        y, t = [], []\n",
    "        for row in cursor:\n",
    "            r_text = row.ttext\n",
    "            r_type = row.ttype\n",
    "            y.append(r_text)\n",
    "            t.append(r_type)\n",
    "\n",
    "            if len(y) == self.batch_size:\n",
    "                npx = np.array(y)\n",
    "                npy = np.array(t)\n",
    "                yield npx, npy\n",
    "                y, t = [], []\n",
    "        pyodbc.Connection.close(cnxn)\n",
    "    \n",
    "    def tokenize(self, file_text):\n",
    "        try:\n",
    "            #firstly let's apply nltk tokenization\n",
    "            #tokens = nltk.word_tokenize(file_text)\n",
    "       \n",
    "            #\\n\n",
    "            #file_text = sub(r'\\n', \" \", file_text)\n",
    "\n",
    "            #print('tokenize ', type(file_text))\n",
    "            tknzr = TweetTokenizer()\n",
    "            tokens = tknzr.tokenize(file_text)\n",
    "            \n",
    "            tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "            tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "            tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        \n",
    "            #let's delete punctuation symbols\n",
    "            stop_words = ([',','\\\\','/','*','','-','http',';',':','@',',','.','#','\"','n','—','_','+','RT'])\n",
    "            tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "            \n",
    "            tokens = [i for i in tokens if not i.isdigit() ]\n",
    "\n",
    "            #deleting stop_words\n",
    "            stop_words = list(set(stopwords.words('russian'))-set(['не','лучше','больше','никогда','хорошо']))\n",
    "            #stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'в', '—', 'к', 'на', 'http', 'чем', 'х','ж','же','\\\\n','\\\\nя'\n",
    "            #                   ,'еще','ещё','d','rt'])\n",
    "\n",
    "            tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "            #cleaning words\n",
    "            #tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "            \n",
    "            return tokens\n",
    "        except:\n",
    "            return 'NC'\n",
    "    \n",
    "    def map_words(self, tokens):\n",
    "        if tokens is not None:\n",
    "            for word in tokens:\n",
    "                word = word.lower()\n",
    "                # Word Exist?\n",
    "                if word in hash_map:\n",
    "                    hash_map[word] = hash_map[word] + 1\n",
    "                else:\n",
    "                    hash_map[word] = 1\n",
    "\n",
    "            return hash_map\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def frequency_mapping(self):\n",
    "        hash_map.clear()\n",
    "\n",
    "        query = \"SELECT [ttext], [ttype] FROM [dbo].[mixedmessages]\"\n",
    "        pdg = self.data_generator(self.cnstr, query)\n",
    "\n",
    "\n",
    "        for current_set in pdg:\n",
    "            for sentence in current_set[0]:\n",
    "                words = self.tokenize(sentence)\n",
    "                self.map = self.map_words(words)\n",
    "        most_popular_words = sorted(self.map, key=self.map.get, reverse=True)[:self.max_words]\n",
    "        indexes = list(range(0, self.max_words))\n",
    "        self.map = dict( zip( most_popular_words, indexes))\n",
    "        #{key: value for key, value in self.map.items() if key in most_popular_words}\n",
    "    \n",
    "    def get_frequency(self, word):\n",
    "        for word in word_list:\n",
    "            print( str(self.map.get(word,0)))\n",
    "    \n",
    "    def vectorize(self,data):\n",
    "        max_frequency = max(self.map.values())\n",
    "        \n",
    "        x_data = []\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            if pd.isnull(row.text):\n",
    "                words = sa.tokenize('')\n",
    "            else:\n",
    "                words = sa.tokenize(row.text)\n",
    "            w = []\n",
    "            #print(words)\n",
    "            for word in words:\n",
    "                #print(word, map.get(word,0))\n",
    "                w.append(self.map.get(word,0))\n",
    "            x_data.append(w)\n",
    "\n",
    "        x_data = np.array(x_data)\n",
    "        x_data = pad_sequences(x_data, maxlen=self.max_len)\n",
    "        #print(x_train)\n",
    "        return x_data\n",
    "    \n",
    "    def tokenize_message(self, string):\n",
    "        words = self.tokenize(string)\n",
    "        res = []\n",
    "        for word in words:\n",
    "            res.append(self.map.get(word,0))\n",
    "        ntm = np.array([res])\n",
    "\n",
    "        nres = pad_sequences(ntm, maxlen=self.max_len)\n",
    "            #res.append([word_2_ind[w] for w in sent.split(' ')])\n",
    "        return nres#np.pad(ntm, (maxlen-len(ntm),0),'constant', constant_values=(0))\n",
    "    \n",
    "    def getRating(self, value):\n",
    "        return {\n",
    "             value < 0.2: 'Крайне негативная оценка',\n",
    "             0.2 <= value < 0.4: 'Негативная оценка',\n",
    "             0.4 <= value < 0.6: 'Нейтральная оценка',\n",
    "             0.6 <= value < 0.8: 'Положительная оценка',\n",
    "             0.8 <= value:       'Крайне положительная оценка'\n",
    "        }[True]\n",
    "    \n",
    "    def sentiment(self, model, text):\n",
    "        inp = np.array(self.tokenize_message(text), dtype=np.int32)\n",
    "        p = model.predict(inp)\n",
    "        return self.getRating(p[0][0]), p[0][0]\n",
    "    \n",
    "    def postprocess(self, data, n=1000000):\n",
    "        data = data.head(n)\n",
    "        \n",
    "        data['tokens'] = data['text'].progress_map(self.tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "        data = data[data.tokens != 'NC']\n",
    "        data.reset_index(inplace=True)\n",
    "        data.drop('index', inplace=True, axis=1)\n",
    "        return data\n",
    "\n",
    "    def frequency_mapping_(self):\n",
    "        hash_map.clear()\n",
    "\n",
    "        query = \"SELECT [ttext], [ttype] FROM [dbo].[mixedmessages]\"\n",
    "        pdg = self.data_generator(self.cnstr, query)\n",
    "\n",
    "\n",
    "        for current_set in pdg:\n",
    "            for sentence in current_set[0]:\n",
    "                words = self.tokenize(sentence)\n",
    "                self.map = self.map_words(words)\n",
    "        most_popular_words = sorted(self.map, key=self.map.get, reverse=True)[:self.max_words]\n",
    "        indexes = list(range(0, self.max_words))\n",
    "        self.map = dict( zip( most_popular_words, indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SentimentAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████████████████████████████████████████████████████| 181467/181467 [00:57<00:00, 3153.67it/s]\n",
      "c:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "progress-bar: 100%|████████████████████████████████████████████████████████████| 22683/22683 [00:07<00:00, 3094.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = sa.postprocess(df_train)\n",
    "val_data = sa.postprocess(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181467</td>\n",
       "      <td>скоро cкоро уже все начнут покупать новогодние...</td>\n",
       "      <td>1</td>\n",
       "      <td>[скоро, cкоро, начнут, покупать, новогодние, ё...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181468</td>\n",
       "      <td>любовь странное чувство,то любишь,то ненавидеш...</td>\n",
       "      <td>0</td>\n",
       "      <td>[любовь, странное, чувство, любишь, ненавидешь...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181469</td>\n",
       "      <td>записывали эфир с большим человеком. реально о...</td>\n",
       "      <td>1</td>\n",
       "      <td>[записывали, эфир, большим, человеком, реально...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181470</td>\n",
       "      <td>химичка,спалила походу(</td>\n",
       "      <td>0</td>\n",
       "      <td>[химичка, спалила, походу, (]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181471</td>\n",
       "      <td>мой хороооший:3 olegmiami5  с днем рождения!))...</td>\n",
       "      <td>1</td>\n",
       "      <td>[хороооший, olegmiami, днем, рождения, !, ), )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  class  \\\n",
       "0  181467  скоро cкоро уже все начнут покупать новогодние...      1   \n",
       "1  181468  любовь странное чувство,то любишь,то ненавидеш...      0   \n",
       "2  181469  записывали эфир с большим человеком. реально о...      1   \n",
       "3  181470                            химичка,спалила походу(      0   \n",
       "4  181471  мой хороооший:3 olegmiami5  с днем рождения!))...      1   \n",
       "\n",
       "                                              tokens  \n",
       "0  [скоро, cкоро, начнут, покупать, новогодние, ё...  \n",
       "1  [любовь, странное, чувство, любишь, ненавидешь...  \n",
       "2  [записывали, эфир, большим, человеком, реально...  \n",
       "3                      [химичка, спалила, походу, (]  \n",
       "4  [хороооший, olegmiami, днем, рождения, !, ), )...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data.tokens)\n",
    "y_train = np.array(train_data['class'])\n",
    "\n",
    "x_test = np.array(val_data.tokens)\n",
    "y_test = np.array(val_data['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding lists of tokens into the word2vec model, we must turn them into LabeledSentence objects beforehand. Here's how to do it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (181443,)\n",
      "x_test shape: (22677,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181443it [00:00, 495751.48it/s]\n",
      "22677it [00:00, 889325.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['пришло', 'время', 'возвращаться', 'мою', 'любимую', 'страну', ':)', 'первое', 'сделаю', 'это', 'проявлю', 'пленку', '!'], tags=['TRAIN_14']),\n",
       " TaggedDocument(words=['никто', 'друзей', 'не', 'пошел', '('], tags=['TRAIN_15']),\n",
       " TaggedDocument(words=['композиция', 'золотые', 'линии', ')'], tags=['TRAIN_16'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[14:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build the word2vec model from x_train i.e. the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 181443/181443 [00:00<00:00, 3024028.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 181443/181443 [00:00<00:00, 3423500.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5177505, 8458660)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v = Word2Vec(size=512, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.84775811e-01,  -2.02515066e-01,  -3.24875116e-01,\n",
       "        -7.58991018e-02,   4.48658943e-01,   2.05963895e-01,\n",
       "         1.60541445e-01,  -1.27083123e-01,  -4.05149348e-03,\n",
       "        -1.12929635e-01,   6.65242150e-02,  -1.54863372e-01,\n",
       "         1.25821903e-01,  -3.53133410e-01,  -2.36356929e-01,\n",
       "         1.28205836e-01,   3.29422802e-01,  -6.69104606e-02,\n",
       "        -7.24445209e-02,  -2.64091879e-01,  -4.01458889e-01,\n",
       "        -2.46305987e-02,  -1.49660498e-01,  -2.73069203e-01,\n",
       "        -6.11310780e-01,   2.77889639e-01,   1.68423966e-01,\n",
       "        -3.01596373e-01,   2.97776848e-01,  -1.87456295e-01,\n",
       "        -6.99449331e-02,  -2.04128295e-01,  -3.16985279e-01,\n",
       "        -1.54065728e-01,  -6.64844224e-03,  -1.56552210e-01,\n",
       "         2.42074251e-01,  -1.88119799e-01,   1.69587031e-01,\n",
       "         1.36620075e-01,  -1.75825313e-01,  -6.03250802e-01,\n",
       "        -2.34590858e-01,  -3.08934301e-01,  -2.85698567e-02,\n",
       "        -7.19273761e-02,  -1.29434377e-01,  -2.18615904e-01,\n",
       "        -2.84732599e-03,   2.03035716e-02,  -1.81194499e-01,\n",
       "         7.55284652e-02,  -1.96084335e-01,  -1.36067986e-01,\n",
       "        -8.32997337e-02,  -1.88325509e-01,  -1.45022914e-01,\n",
       "        -7.99031556e-03,  -9.89371762e-02,  -4.89606529e-01,\n",
       "         5.58045059e-02,  -1.67565286e-01,  -2.91551143e-01,\n",
       "        -4.13151681e-01,   2.79489569e-02,   7.41173401e-02,\n",
       "         7.74500608e-01,   2.33549908e-01,   5.98789863e-02,\n",
       "        -2.65480340e-01,  -1.22412611e-02,  -2.32743114e-01,\n",
       "         2.27559596e-01,  -2.00935751e-01,   4.67331290e-01,\n",
       "        -2.52413690e-01,   1.54888809e-01,  -6.99158236e-02,\n",
       "        -1.04120150e-01,   4.56957996e-01,   2.11403687e-02,\n",
       "         4.08402860e-01,  -1.45079195e-01,   7.09027573e-02,\n",
       "        -1.27756178e-01,  -8.66872445e-02,   1.75743237e-01,\n",
       "         2.70939440e-01,  -1.20645784e-01,   1.80009320e-01,\n",
       "         4.30073217e-02,   3.19005735e-02,  -3.23902488e-01,\n",
       "        -4.77684587e-02,  -7.40846246e-03,  -5.23243099e-02,\n",
       "        -2.62646098e-02,   3.02991033e-01,   3.71694677e-02,\n",
       "        -2.41473988e-01,   3.07574123e-01,  -1.54046506e-01,\n",
       "         2.83811957e-01,   2.44973317e-01,   2.46163338e-01,\n",
       "        -2.69470245e-01,   1.46930143e-01,   1.53968692e-01,\n",
       "         3.06447712e-03,  -4.67500359e-01,  -3.86654198e-01,\n",
       "         1.60409957e-01,   1.81937143e-02,   1.34376466e-01,\n",
       "         6.86639696e-02,   1.03919394e-01,   6.08745664e-02,\n",
       "         1.79343298e-02,   9.14641283e-03,  -2.49802187e-01,\n",
       "         4.37090583e-02,  -4.53138173e-01,  -1.62822336e-01,\n",
       "        -1.86414137e-01,  -5.84913015e-01,   3.82548757e-02,\n",
       "        -1.15662903e-01,   1.38540193e-01,  -2.64642388e-01,\n",
       "        -3.51208746e-01,  -1.60022974e-01,  -4.67453301e-02,\n",
       "        -8.78627673e-02,   2.12548926e-01,  -2.85912473e-02,\n",
       "        -2.89481115e-02,  -4.67189521e-01,   1.02708794e-01,\n",
       "         5.03349416e-02,   4.30509038e-02,   2.23583654e-01,\n",
       "        -1.72121704e-01,   2.82285381e-02,  -2.41815925e-01,\n",
       "        -9.84156281e-02,   1.58579964e-02,  -1.36690527e-01,\n",
       "         1.08545579e-01,  -5.50347984e-01,   1.56060934e-01,\n",
       "        -1.22135602e-01,  -1.60475791e-01,  -3.77039984e-02,\n",
       "        -2.54891932e-01,  -2.40334764e-01,  -6.20425493e-02,\n",
       "         5.45969963e-01,  -4.59422648e-01,   3.21720868e-01,\n",
       "         1.92559078e-01,   3.07584077e-01,   6.61250502e-02,\n",
       "        -8.26648921e-02,  -1.12101838e-01,   3.78578633e-01,\n",
       "         9.43776742e-02,   2.75163412e-01,   2.35274196e-01,\n",
       "         5.01138791e-02,   8.51350948e-02,   5.40556386e-02,\n",
       "         2.31497392e-01,   2.99486786e-01,  -1.80044368e-01,\n",
       "         1.72773883e-01,  -8.73922929e-02,  -4.09724474e-01,\n",
       "         2.47849002e-01,  -7.58847222e-02,   2.25428015e-01,\n",
       "         1.69346169e-01,   4.80026491e-02,   5.09426773e-01,\n",
       "         2.57464051e-01,  -2.62299240e-01,  -7.13733658e-02,\n",
       "        -1.41578168e-02,   5.19056953e-02,  -4.59311455e-01,\n",
       "         6.75402880e-01,   4.50062424e-01,  -2.49269485e-01,\n",
       "         4.52399589e-02,   1.29427806e-01,   6.98370263e-02,\n",
       "         1.07078724e-01,   4.65938449e-01,   1.41506633e-02,\n",
       "        -2.30245367e-01,  -4.31573600e-01,   9.29301754e-02,\n",
       "        -2.30482385e-01,   2.21332178e-01,   1.06478669e-01,\n",
       "         5.86942583e-02,   4.64485496e-01,   1.50674894e-01,\n",
       "        -4.52460617e-01,   2.51140565e-01,  -1.54054433e-01,\n",
       "         4.35168117e-01,   1.09427923e-03,  -3.39588493e-01,\n",
       "         1.78812668e-01,   7.79217109e-03,  -3.40703905e-01,\n",
       "         1.21823326e-01,  -6.94816485e-02,   2.06753880e-01,\n",
       "         1.99306175e-01,   7.58101642e-02,   1.01159431e-01,\n",
       "        -4.42467690e-01,   1.47116318e-01,  -3.93466018e-02,\n",
       "         4.47624743e-01,  -6.41243756e-01,  -2.25809410e-01,\n",
       "         1.61702707e-01,   2.34883189e-01,   8.34961161e-02,\n",
       "        -1.11956345e-02,   2.38934636e-01,   1.56593680e-01,\n",
       "         2.51586109e-01,   6.24783337e-04,  -3.93127322e-01,\n",
       "         3.15666199e-01,  -1.18336149e-01,  -3.56616050e-01,\n",
       "        -3.10361199e-03,  -1.26352943e-02,   1.29565403e-01,\n",
       "        -1.01494245e-01,   1.19260460e-01,  -5.05263396e-02,\n",
       "         5.02977014e-01,   3.14178318e-01,  -1.39774919e-01,\n",
       "        -7.35568106e-02,   4.19110864e-01,  -2.01756462e-01,\n",
       "        -7.45793059e-02,   8.67764801e-02,   4.55926150e-01,\n",
       "         4.75314911e-03,   2.73521483e-01,   1.97504759e-01,\n",
       "         3.06745023e-01,  -1.73705295e-01,   3.99850719e-02,\n",
       "        -1.41516671e-01,  -2.18288302e-01,   8.32696706e-02,\n",
       "         1.21222377e-01,  -5.31734899e-02,   4.44056652e-02,\n",
       "         8.22651759e-02,   1.38653472e-01,   2.35504255e-01,\n",
       "        -1.08187320e-03,   2.89297223e-01,  -1.84434727e-01,\n",
       "        -3.34201038e-01,  -2.78948188e-01,   2.88351476e-01,\n",
       "        -4.90170836e-01,   2.11715579e-01,  -2.44783625e-01,\n",
       "         7.31762201e-02,   3.19196165e-01,  -5.04196547e-02,\n",
       "        -3.57755162e-02,   3.73492241e-01,   4.36302088e-02,\n",
       "         9.93861482e-02,   3.25231582e-01,  -4.25385386e-01,\n",
       "        -1.74600139e-01,   8.35429803e-02,   7.78985023e-02,\n",
       "        -4.88416910e-01,  -8.92839879e-02,   6.98554292e-02,\n",
       "        -1.77252330e-02,  -3.11312258e-01,   1.74524933e-01,\n",
       "        -2.31182352e-01,  -6.40498707e-03,  -1.17985606e-01,\n",
       "         1.06373794e-01,   1.15127422e-01,   1.69773743e-01,\n",
       "        -1.40893325e-01,   4.62600701e-02,   2.73054063e-01,\n",
       "         2.69853801e-01,   3.73908520e-01,  -1.41203895e-01,\n",
       "        -3.45715761e-01,   2.66455740e-01,   1.67307794e-01,\n",
       "         6.91781342e-01,   2.53994107e-01,  -2.54004836e-01,\n",
       "         6.52025491e-02,   4.81391177e-02,   1.86058968e-01,\n",
       "         1.77413404e-01,  -3.20558906e-01,   2.17305064e-01,\n",
       "         1.16777718e-01,  -6.44615814e-02,  -2.60783702e-01,\n",
       "         8.91073495e-02,  -3.98091167e-01,  -6.21477626e-02,\n",
       "         1.32484555e-01,   7.13021532e-02,   1.43926054e-01,\n",
       "         1.10354558e-01,   4.20359662e-03,   4.04504478e-01,\n",
       "        -7.02450722e-02,   1.41900182e-01,   3.06813836e-01,\n",
       "         3.91068645e-02,  -2.39552155e-01,  -3.09943199e-01,\n",
       "         1.08093649e-01,  -1.22430183e-01,   2.00646594e-01,\n",
       "         2.84581602e-01,   8.47661693e-04,   2.37036318e-01,\n",
       "        -3.66981447e-01,   2.00322747e-01,  -4.64407206e-01,\n",
       "        -1.09520860e-01,   7.25432262e-02,  -1.53623462e-01,\n",
       "        -2.52725095e-01,   1.77305162e-01,  -3.24513495e-01,\n",
       "        -1.65332079e-01,   3.13777685e-01,   5.75173199e-02,\n",
       "        -4.39115949e-02,   6.16627075e-02,   1.13964334e-01,\n",
       "        -2.41767451e-01,  -8.85185450e-02,   3.27903517e-02,\n",
       "         2.17384622e-02,   1.20731443e-01,  -9.28641483e-02,\n",
       "         3.02686334e-01,   1.70891907e-03,  -6.60465285e-02,\n",
       "         1.41418710e-01,  -2.76398718e-01,   9.58954915e-02,\n",
       "         2.66610295e-01,  -2.56311119e-01,  -2.85718620e-01,\n",
       "         9.58554670e-02,  -4.01783735e-02,   2.41207466e-01,\n",
       "         1.32166907e-01,   3.14968735e-01,  -4.27082121e-01,\n",
       "         6.00969279e-03,  -3.21802825e-01,   4.32562053e-01,\n",
       "         2.28607491e-01,   2.71126568e-01,  -1.55566171e-01,\n",
       "        -6.89456472e-03,   3.38428974e-01,   1.52462140e-01,\n",
       "        -2.87388444e-01,   1.09934561e-01,  -3.56869340e-01,\n",
       "         2.22679719e-01,   1.65219575e-01,   8.32449272e-02,\n",
       "        -1.42770916e-01,   1.16688758e-01,   2.77126372e-01,\n",
       "         3.48645896e-01,   4.57049310e-01,   3.42160970e-01,\n",
       "         1.38535395e-01,  -2.81201098e-02,  -2.05419660e-01,\n",
       "        -3.05484086e-01,   6.91402406e-02,   3.46140534e-01,\n",
       "        -3.97960134e-02,   1.23793356e-01,   1.43474340e-02,\n",
       "         6.55311197e-02,   1.86844230e-01,  -1.85463727e-01,\n",
       "         3.68828624e-01,   5.71528040e-02,  -3.42508525e-01,\n",
       "         4.94599015e-01,  -1.87020019e-01,  -4.29760776e-02,\n",
       "         2.98773378e-01,  -4.11465555e-01,   1.35876425e-02,\n",
       "        -2.49399081e-01,  -2.11832851e-01,   1.96396083e-01,\n",
       "         5.64148486e-01,   7.43686706e-02,  -2.28807777e-02,\n",
       "         5.06337285e-01,   1.51403129e-01,   3.04376066e-01,\n",
       "         4.76495326e-02,  -2.93695807e-01,   1.31353378e-01,\n",
       "        -3.51663888e-03,   6.82529015e-03,   2.64122307e-01,\n",
       "        -2.85922766e-01,   8.10646415e-02,  -3.28759432e-01,\n",
       "        -2.29458705e-01,  -1.07988104e-01,   5.95814645e-01,\n",
       "        -2.24534035e-01,   2.18895063e-01,  -3.63246650e-01,\n",
       "         2.23622888e-01,   3.06959659e-01,   9.92560163e-02,\n",
       "        -5.79786420e-01,  -2.78516263e-01,   5.38555503e-01,\n",
       "        -2.41751522e-01,   4.39170510e-01,  -2.18339741e-01,\n",
       "         1.63990352e-02,   1.08108118e-01,  -2.06396237e-01,\n",
       "         4.55682874e-01,  -2.51032948e-01,  -3.90529007e-01,\n",
       "        -1.92165792e-01,  -3.29320468e-02,  -1.89955696e-01,\n",
       "         1.66011855e-01,  -1.00412816e-01,   6.57933112e-03,\n",
       "        -1.51001021e-01,  -5.36601663e-01,  -4.27592337e-01,\n",
       "        -2.48952687e-01,   2.85309494e-01,   3.88181388e-01,\n",
       "         1.16460443e-01,   7.35543892e-02,   2.75977999e-01,\n",
       "        -1.75208092e-01,   1.40817706e-02,  -5.25686517e-02,\n",
       "         4.16247219e-01,  -2.25150902e-02,   2.80468196e-01,\n",
       "        -2.66866051e-02,  -2.19746739e-01,   5.08190086e-03,\n",
       "         1.20594800e-01,   1.65405944e-01,   2.03787852e-02,\n",
       "         1.55758389e-05,  -2.79641867e-01,  -2.13053495e-01,\n",
       "        -1.15851291e-01,  -7.18405172e-02,   3.32535058e-01,\n",
       "        -1.94714606e-01,   2.45644636e-02,   6.76551685e-02,\n",
       "         2.79362835e-02,   1.21458478e-01,   3.69819179e-02,\n",
       "        -4.68334943e-01,   1.14449739e-01,   1.93341389e-01,\n",
       "        -1.47233143e-01,  -2.57681996e-01,  -6.09271750e-02,\n",
       "         3.66231620e-01,  -1.66207194e-01,   1.16219461e-01,\n",
       "        -1.77677333e-01,  -1.40934676e-01], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.wv['хороший']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('крутой', 0.9412223100662231),\n",
       " ('классный', 0.9401661157608032),\n",
       " ('добрый', 0.8919504880905151),\n",
       " ('шикарный', 0.8890976905822754),\n",
       " ('замечательный', 0.8841381072998047),\n",
       " ('приятный', 0.8748196363449097),\n",
       " ('прекрасный', 0.8747985363006592),\n",
       " ('счастливый', 0.8644788861274719),\n",
       " ('офигенный', 0.8638586401939392),\n",
       " ('отличный', 0.851129412651062)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.wv.most_similar('хороший')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сериал', 0.9124467372894287),\n",
       " ('класс', 0.7722108960151672),\n",
       " ('сезон', 0.701657772064209),\n",
       " ('момент', 0.6966123580932617),\n",
       " ('вечер', 0.6921699047088623),\n",
       " ('офигенский', 0.6899518966674805),\n",
       " ('понравился', 0.6898279190063477),\n",
       " ('используем', 0.6867188215255737),\n",
       " ('смотрела', 0.6815809607505798),\n",
       " ('посмотрел', 0.6779988408088684)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.wv.most_similar('фильм')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181467, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((df_train.shape[0], max_len, 512), dtype=K.floatx())\n",
    "Y_train = np.zeros((df_train.shape[0]), dtype=np.int32)\n",
    "X_test = np.zeros((df_test.shape[0], max_len, 512), dtype=K.floatx())\n",
    "Y_test = np.zeros((df_test.shape[0]), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, sa.max_words) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, sa.max_words) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_vecs_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model sequentially ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_5: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-a9a77afd2307>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Building model sequentially ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vecs_w2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_5: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "print('Building model sequentially ...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(train_vecs_w2v.shape[0],), dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.recurrent.LSTM at 0x2245e353668>,\n",
       " <keras.layers.core.Dense at 0x2245e3536d8>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "- class_name: LSTM\n",
      "  config:\n",
      "    activation: tanh\n",
      "    activity_regularizer: null\n",
      "    batch_input_shape: !!python/tuple [null, 181443, 2000]\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    dropout: 0.2\n",
      "    dtype: float32\n",
      "    go_backwards: false\n",
      "    implementation: 1\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: lstm_2\n",
      "    recurrent_activation: hard_sigmoid\n",
      "    recurrent_constraint: null\n",
      "    recurrent_dropout: 0.2\n",
      "    recurrent_initializer:\n",
      "      class_name: Orthogonal\n",
      "      config: {gain: 1.0, seed: null}\n",
      "    recurrent_regularizer: null\n",
      "    return_sequences: false\n",
      "    return_state: false\n",
      "    stateful: false\n",
      "    trainable: true\n",
      "    unit_forget_bias: true\n",
      "    units: 128\n",
      "    unroll: false\n",
      "    use_bias: true\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: sigmoid\n",
      "    activity_regularizer: null\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_1\n",
      "    trainable: true\n",
      "    units: 1\n",
      "    use_bias: true\n",
      "keras_version: 2.1.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dot': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\dot.exe', 'twopi': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\twopi.exe', 'neato': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\neato.exe', 'circo': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\circo.exe', 'fdp': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\fdp.exe', 'sfdp': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\sfdp.exe'}\n"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "import pydot_ng as pydot\n",
    "print (pydot.find_graphviz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"221pt\" viewBox=\"0.00 0.00 359.00 221.00\" width=\"359pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-217 355,-217 355,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2355222624520 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2355222624520</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 351,-212.5 351,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"78.5\" y=\"-185.8\">lstm_2_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"157,-166.5 157,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"157,-189.5 213,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"213,-166.5 213,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282\" y=\"-197.3\">(None, 181443, 2000)</text>\n",
       "<polyline fill=\"none\" points=\"213,-189.5 351,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282\" y=\"-174.3\">(None, 181443, 2000)</text>\n",
       "</g>\n",
       "<!-- 2355222623848 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2355222623848</title>\n",
       "<polygon fill=\"none\" points=\"29.5,-83.5 29.5,-129.5 321.5,-129.5 321.5,-83.5 29.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"78.5\" y=\"-102.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"127.5,-83.5 127.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"127.5,-106.5 183.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"183.5,-83.5 183.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-114.3\">(None, 181443, 2000)</text>\n",
       "<polyline fill=\"none\" points=\"183.5,-106.5 321.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-91.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2355222624520&#45;&gt;2355222623848 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2355222624520-&gt;2355222623848</title>\n",
       "<path d=\"M175.5,-166.366C175.5,-158.152 175.5,-148.658 175.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"179,-139.607 175.5,-129.607 172,-139.607 179,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2355222623960 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2355222623960</title>\n",
       "<polygon fill=\"none\" points=\"54,-0.5 54,-46.5 297,-46.5 297,-0.5 54,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"158,-0.5 158,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"158,-23.5 214,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"214,-0.5 214,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-31.3\">(None, 128)</text>\n",
       "<polyline fill=\"none\" points=\"214,-23.5 297,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2355222623848&#45;&gt;2355222623960 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2355222623848-&gt;2355222623960</title>\n",
       "<path d=\"M175.5,-83.3664C175.5,-75.1516 175.5,-65.6579 175.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"179,-56.6068 175.5,-46.6068 172,-56.6069 179,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.objectives import binary_crossentropy\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (181443, 2000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0fdfcba5391e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     callbacks=[tensorboard, early_stopping])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1581\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1582\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gorda\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    139\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (181443, 2000)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard  \n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=3)  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=sa.batch_size,\n",
    "                    epochs=sa.epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22683/22683 [==============================] - 1s 37us/step\n",
      "\n",
      "\n",
      "Test score: 0.0650629415337\n",
      "Test accuracy: 0.986686064454\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=sa.batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22684/22684 [==============================] - 1s 27us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(x_test, batch_size=sa.batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98367172,  0.01632828]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sa.sentiment(model,'Мне не нравится фильм. Сюжет совсем неинтересный и актеры сыграли плохо')\n",
    "\n",
    "#x_text = pad_sequences(x_text, maxlen=sa.max_len)\n",
    "\n",
    "#model.predict(x_text)\n",
    "inp = np.array(sa.tokenize_message('Мне нравится фильм. Отличная игра актеров и интересный сюжет'), dtype=np.int32)\n",
    "p = model.predict(inp)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-d3ee256068ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_train shape:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_text' is not defined"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Делаю пробное печенье по рецепту makeupkaty , пока что без формы) http://t.co/bRZjtMdXyd'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
